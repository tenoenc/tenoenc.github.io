---
layout: post
title: "[OS] I/O Models"
date: 2026-01-10 19:52 +0900
math: true
categories:
- Computer Science
- Operating System & Computer Structure
tags:
- Operating System
- Blocking
- NonBloking
- Synchronous
- Asynchronous
image:
    path: /assets/img/2026-01-13-16-36-32.png
---

## 🧩 I/O Model

우리가 작성한 코드가 하드웨어의 물리적 실체에 닿기까지는 운영체제라는 거대한 중재자가 놓여 있습니다. 이 인위적인 세계에서 애플리케이션은 스스로 데이터를 읽거나 쓸 권한이 없으며, 오직 '시스템 콜(System Call)'이라는 엄격한 통로를 통해 커널(Kernel)에게 작업을 대행해 달라고 요청해야 합니다.

### 프로세스의 추상화

컴퓨터의 가장 중요한 두 하드웨어 구성 요소인 CPU와 RAM은 각각 연산의 뇌와 단기 기억 장치로서 마더보드를 통해 긴밀히 통신합니다. 운영체제의 본질적인 업무는 이 물리적인 하드웨어들 위에 **프로세스(Process)**라는 근본적인 추상화 계층을 구축하는 것입니다. 우리가 사용하는 모든 소프트웨어는 이 프로세스 내부에서 실행되며, 이는 하드웨어의 복잡한 물리적 동작을 감추고 소프트웨어가 안전하게 자원을 점유할 수 있도록 보호된 울타리를 제공합니다.

운영체제가 제공하는 이 가상의 세계는 강력한 멀티태스킹 기능을 통해 우리에게 마치 여러 프로그램이 동시에 실행되는 것 같은 환상을 심어줍니다. 실제로는 하나의 CPU 코어가 수 밀리초 단위로 프로세스를 번갈아 가며 실행하는 것이지만, CPU는 초당 수십억 개의 명령어를 처리할 만큼 압도적으로 빠르기에 인간은 그 틈새를 인지하지 못합니다. 이러한 정교한 시간 쪼개기(Time-slicing) 기술 덕분에 우리는 하드웨어의 물리적 개수보다 훨씬 많은 수의 작업을 동시에 처리하는 논리적 풍요로움을 누리게 됩니다.

하지만 이 가상의 풍요로움은 결코 공짜가 아닙니다. 프로세스를 전환할 때마다 운영체제는 현재 실행 중인 프로세스의 레지스터 상태를 저장하고 커널 모드로 진입하며 가상 메모리를 관리하는 등의 복잡한 뒷정리를 수행해야 합니다. 이러한 전환 작업이 현대적인 시스템에서도 상당한 성능 비용을 발생시킵니다. 결국 I/O 모델을 깊게 이해한다는 것은, 운영체제가 구축한 프로세스라는 추상화 이면에서 하드웨어 자원이 실제로 어떻게 배분되고 소모되는지를 꿰뚫어 보는 과정과 같습니다.

### 왜 지연되는가

엔지니어링 관점에서 I/O는 '지연과의 전쟁'입니다. CPU가 초당 수십억 개의 명령어를 처리하는 동안, 디스크나 네트워크는 그보다 수만 배 느린 속도로 반응합니다.

* **CPU 유휴 시간**: 하드 드라이브에서 데이터를 읽어오는 것과 같은 외부 이벤트를 기다리는 동안, CPU는 아무런 유용한 작업도 하지 못한 채 '대기(Waiting)' 상태로 전이됩니다.
* **문맥 전환(Context Switching)**: 실행 중인 프로세스가 I/O를 기다리며 자리를 비울 때 발생하는 전환 비용은 약 **5마이크로초**에 달하며, 이는 수만 개의 명령어 실행 기회를 소비하는 것과 같습니다.

### 데이터 복사와 메모리 경계선

단순히 기다림만이 문제가 아닙니다. 데이터가 하드웨어에서 애플리케이션까지 전달되는 과정에는 '데이터 복사(Data Copy)'라는 숨은 비용이 존재합니다.

1.  **커널 버퍼 유입**: 하드웨어 장치 드라이버가 읽어온 데이터가 커널 영역의 메모리에 먼저 채워집니다.
2.  **유저 영역 복사**: 커널은 자신의 버퍼에 있는 데이터를 다시 애플리케이션의 유저 영역 메모리로 복사합니다.

이 이중적인 복사 과정은 메모리 대역폭을 소모하고 CPU 부하를 동반합니다. 결국 고성능 시스템을 설계한다는 것은 이 데이터의 흐름과 CPU의 대기 상태를 얼마나 정교하게 장악하느냐에 달려 있습니다.

> **I/O 모델 선택의 본질**
> 
> 어떤 I/O 모델을 선택하느냐는 결국 **"CPU가 외부 데이터가 올 때까지 문을 잠그고 기다릴 것인가, 아니면 다른 일을 하며 주기적으로 확인할 것인가"**에 대한 전략적 결정입니다.
{: .prompt-tip }

## 🧩 Blocking

가장 직관적이며 원초적인 이 모델은 '기다림'을 엔지니어링의 중심에 둡니다. 우리가 작성한 애플리케이션 코드가 소켓에서 `read()`를 호출하는 순간, 커널 버퍼에 데이터가 준비되어 있지 않다면 실행 흐름은 그 자리에서 즉시 멈춰 서게 됩니다. 이는 단순히 코드의 한 줄이 멈추는 것을 넘어, 운영체제 수준에서 프로세스의 생명 주기가 일시적으로 중단됨을 의미합니다. 운영체제의 핵심 역할은 하드웨어 자원을 추상화하여 관리하는 것이며, Blocking 호출 시 커널은 해당 프로세스를 '실행(Running)' 상태에서 '대기(Waiting)' 상태로 전이시켜 CPU 스케줄링 대상에서 완전히 제외합니다.

이 과정에서 커널은 프로세스의 문맥(Context)을 저장하고, 프로세스 제어 블록(PCB)을 실행 큐에서 제거하여 특정 장치의 `wait_queue`로 이동시킵니다. CPU는 이제 할 일이 없어진 대기 프로세스를 떠나 다른 실행 가능한 작업을 찾아 떠나는데, 이 문맥 전환 과정에서 발생하는 지연이 약 5마이크로초에 달합니다. 이는 CPU가 수만 개의 명령어를 처리할 수 있는 천금 같은 기회를 단순히 '작업 환경을 교체하는 데' 써버리는 것과 같습니다.

![](/assets/img/2026-01-10-20-36-09.png)

이러한 하드웨어와 커널 사이의 복잡한 상호작용, 즉 시스템 콜 호출부터 트랩(Trap) 발생, 커널 버퍼 확인, 프로세스 수면 상태 진입, 그리고 인터럽트에 의한 깨어남으로 이어지는 일련의 메커니즘은 아래의 코드 구조 속에 상세히 녹아 있습니다.

```c
#include <stdio.h>
#include <unistd.h>
#include <sys/socket.h>
#include <arpa/inet.h>

/**
 * C 기반 Blocking 소켓 I/O 메커니즘
 * * 이 코드는 가장 전형적인 Blocking 모델을 보여줍니다.
 * 운영체제는 하드웨어 자원을 보호하기 위해 
 * 프로세스에 직접적인 제어권을 주지 않고 '시스템 콜'이라는 중재안을 제시합니다.
 */
void start_blocking_server(int client_fd) {
    char buffer[1024];

    printf("데이터 수신 대기 중...\n");

    /* * 1. 시스템 콜 진입 (Trap):
     * 유저 모드에서 커널 모드로 실행 권한이 전환됩니다.
     * * 2. 커널 버퍼 확인:
     * 커널은 해당 소켓의 수신 버퍼에 데이터가 있는지 확인합니다.
     * * 3. 프로세스 잠들기 (Waiting State):
     * 데이터가 없다면, 커널은 이 스레드의 PCB(Process Control Block)를 
     * 실행 큐에서 제거하고 'Wait Queue'로 이동시킵니다.
     * * 4. 문맥 전환 (Context Switch):
     * CPU는 현재 스레드의 레지스터 상태를 저장하고 다른 작업을 수행하러 떠납니다. 
     * 이 전환에 약 5마이크로초가 소요됩니다.
     */
    ssize_t bytes_read = read(client_fd, buffer, sizeof(buffer)); 

    /*
     * 5. 인터럽트 및 깨어남 (Ready State):
     * 장치 드라이버가 데이터를 수신하면 CPU에 인터럽트를 보내고, 
     * 커널은 잠들었던 프로세스를 다시 'Ready' 상태로 전환합니다.
     * * 6. 데이터 복사 및 복귀:
     * 커널 버퍼의 데이터가 유저 영역의 buffer[]로 복사된 후, 
     * 비로소 read() 함수가 반환되며 제어권이 다시 유저에게 돌아옵니다.
     */
    if (bytes_read > 0) {
        printf("수신 데이터: %.*s\n", (int)bytes_read, buffer);
    }

    close(client_fd);
}
```

Blocking 모델의 진정한 위협은 단순한 시간 지연보다 '자원 오염'에 가깝습니다. 프로세스가 CPU에서 쫓겨날 때, 해당 프로세스가 사용하던 L1, L2 캐시 데이터는 무효화(Flush)됩니다. 이후 데이터가 도착하여 프로세스가 다시 복귀했을 때 직면하는 '캐시 미스(Cache Miss)'는 시스템의 전체적인 처리량(Throughput)을 갉아먹는 주범이 됩니다. 또한, 고전적인 '연결당 스레드 하나' 모델을 사용할 경우, 수만 개의 스레드가 각자 Blocking 상태로 잠들게 되면 커널은 이들을 관리하고 전환하는 비용만으로 전체 자원의 상당 부분을 탕진하게 됩니다.

> **Blocking I/O의 구현적 가치**
> 
> Blocking 모델은 성능 최적화 관점에서는 불리할 수 있으나, 비즈니스 로직의 순차적 흐름과 코드의 실행 흐름이 1:1로 일치한다는 강력한 강점을 가집니다. 이는 디버깅의 용이성과 코드의 명확성을 보장하므로, 동시성보다는 데이터 처리의 정확성과 선후 관계가 핵심인 금융권의 배치 작업이나 트랜잭션 도메인에서 여전히 표준적인 전략으로 사용됩니다.
{: .prompt-warning }

## 🧩 Non-Blocking

Non-Blocking 모델은 시스템 콜의 반환 시점을 비틀어 Blocking의 고질적인 문제인 '대기 상태로의 전이'를 정면으로 거부합니다. 애플리케이션이 커널에 데이터를 요청했을 때, 준비된 데이터가 없다면 커널은 프로세스를 잠재우는 대신 즉시 에러 코드(`EAGAIN` 혹은 `EWOULDBLOCK`)를 유저 스페이스로 던집니다. 운영체제의 핵심 역할은 프로세스가 자원을 효율적으로 쓰게 돕는 것이며, Non-Blocking 호출은 프로세스가 '대기(Waiting)' 상태로 빠지지 않고 '실행(Running)' 상태를 유지하게 만드는 결정적인 동력이 됩니다. 프로세스는 제어권을 잃지 않았기에 멈추지 않고 다음 연산을 수행하거나 또 다른 I/O 요청을 보내며 CPU 자원을 물리적으로 점유할 수 있는 권리를 확보합니다.

![](/assets/img/2026-01-13-16-28-03.png)

이러한 자유의 대가는 '반복적인 질의(Polling)'라는 형벌로 돌아옵니다. 데이터가 언제 준비될지 알 수 없는 애플리케이션은 커널에게 상태를 끊임없이 재확인해야 합니다. 이러한 바쁜 대기(Busy Wait)는 CPU 사용률을 무의미하게 치솟게 만듭니다. 실제로 데이터가 도착하지 않은 찰나의 순간에도 CPU는 루프를 돌며 시스템 콜을 호출해야 하며, 이는 유저 모드와 커널 모드를 오가는 '모드 전환(Mode Switching)' 비용을 발생시킵니다. 컨텍스트 스위칭 비용이 프로세스 간의 물리적인 전환 비용이라면, Non-Blocking의 Polling은 '빈 서랍을 수만 번 열어보는 유저/커널 경계 왕복 비용'에 비유할 수 있습니다.

이러한 하드웨어와 커널 사이의 상호작용, 특히 제어권의 즉시 반환과 `EAGAIN` 응답을 처리하며 CPU가 쉬지 않고 회전(Spin)하는 과정은 아래의 코드 구조 속에 상세히 녹아 있습니다.

```c
#include <stdio.h>
#include <unistd.h>
#include <fcntl.h>
#include <errno.h>
#include <sys/socket.h>

/**
 * Non-Blocking 소켓 설정 및 Polling 메커니즘
 * * Non-Blocking 모델의 핵심은 '기다리지 않는 용기'와 '끊임없는 확인'에 있습니다.
 * 프로세스는 Waiting 상태로 전이되지 않고 
 * 계속 CPU를 점유하며 로직을 수행할 수 있는 권리를 유지합니다.
 */
void start_non_blocking_polling(int sockfd) {
    char buffer[1024];

    // 1. 소켓을 Non-Blocking 모드로 전환합니다.
    // 이제 이 소켓을 대상으로 하는 시스템 콜은 제어권을 즉시 반환합니다.
    int flags = fcntl(sockfd, F_GETFL, 0);
    fcntl(sockfd, F_SETFL, flags | O_NONBLOCK);

    printf("비차단 모드에서 데이터를 주기적으로 확인합니다 (Polling)...\n");

    while (1) {
        /*
         * 2. 제어권 즉시 반환 (Immediate Return):
         * read() 호출 시 데이터가 없다면 커널은 프로세스를 잠재우지 않습니다.
         * 대신 EAGAIN 또는 EWOULDBLOCK 에러를 즉시 반환하며 제어권을 돌려줍니다.
         */
        ssize_t bytes_read = read(sockfd, buffer, sizeof(buffer));

        if (bytes_read == -1) {
            if (errno == EAGAIN || errno == EWOULDBLOCK) {
                /*
                 * 3. 바쁜 대기 (Busy Wait/Spinning):
                 * 데이터가 아직 도착하지 않았습니다. 프로세스는 여기서 멈추지 않고 
                 * 다른 연산을 수행하거나, 다시 시스템 콜을 호출할 준비를 합니다.
                 * * 주의: 이 루프는 CPU 사용률을 100%까지
                 * 치솟게 만들며 유저/커널 경계를 왕복하는 모드 전환 비용을 발생시킵니다.
                 */
                printf("데이터 미도착. 다른 작업 수행 중...\n");
                
                // 실제 고성능 서버라면 여기서 다른 유의미한 작업을 수행합니다.
                do_other_business_logic(); 
                
                continue; 
            }
            break; // 실제 에러 발생 시 루프 종료
        }

        /*
         * 4. 데이터 획득:
         * 여러 번의 Polling 끝에 데이터가 버퍼에 차는 순간, 
         * 시스템 콜은 비로소 유의미한 바이트 수를 반환합니다.
         */
        if (bytes_read > 0) {
            printf("수신 성공: %.*s\n", (int)bytes_read, buffer);
            break;
        }
    }
}
```

> **Non-Blocking 모델의 효율적 임계점**
> 
> Non-Blocking I/O는 단일 스레드가 수천 개의 연결을 동시에 관리할 수 있는 아키텍처적 기반을 제공하지만, Polling의 밀도가 높아질수록 CPU 효율은 급격히 하락합니다. 따라서 실제 엔지니어링에서는 이를 단독으로 쓰기보다 커널이 이벤트 발생을 알려주는 멀티플렉싱 기술과 결합하여, CPU가 '유의미한 신호'가 있을 때만 움직이도록 설계하는 것이 핵심입니다.
{: .prompt-info }

## 🧩 Synchronous

Synchronous(동기) 모델은 '작업의 완료를 기다리는 주체'가 누구인지, 그리고 제어 흐름과 데이터의 흐름이 어떻게 맞물리는지에 집중합니다. 이 모델에서 애플리케이션은 커널에게 요청한 작업이 끝날 때까지 그 결과값을 직접 챙겨야 할 의무가 있습니다. 많은 이들이 Blocking과 Synchronous를 혼동하지만, 이 둘은 엄밀히 다른 층위의 개념입니다. 프로세스는 운영체제가 제공하는 명령어의 순차적 실행 단위이며, 동기 모델은 이 순차성을 논리적으로 강제하는 방식입니다. 설령 Non-Blocking 호출을 통해 제어권을 즉시 돌려받았다 하더라도, 애플리케이션이 루프를 돌며 데이터가 도착했는지 직접 확인하고 있다면 이는 여전히 동기식 흐름에 해당합니다. 실행 흐름(Control Flow)과 데이터의 완성(Data Completion)이 논리적으로 강하게 결합되어 있기 때문입니다.

프로세스 추상화 관점에서 동기 모델은 인과관계가 명확한 직렬적 세계를 구축합니다. 하나의 요청이 발생하고, 그에 따른 작업이 수행되며, 결과가 반환되는 과정이 단일한 선형적 타임라인 위에 놓입니다. 이러한 구조는 개발자에게 강력한 예측 가능성을 제공하지만, 시스템의 전체 처리 속도가 가장 느린 구성 요소인 I/O 장치의 성능에 종속되는 병목 현상을 야기합니다. CPU가 초당 수십억 개의 명령어를 처리할 수 있음에도 불구하고, 동기적 구조 아래서는 외부 데이터가 준비되어 유저 영역으로 복사될 때까지 로직의 다음 단계로 나아가지 못하고 묶여 있게 됩니다.

이러한 논리적 동기화 과정, 즉 호출자가 피호출자의 상태를 주기적으로 확인하며 작업의 종결점을 스스로 찾아내는 메커니즘은 아래의 코드 구조에서 명확히 드러납니다. 제어권을 돌려받는 행위와 결과값을 확정 짓는 행위 사이의 간극을 어떻게 메우는지가 동기 모델의 핵심입니다.

```c
/**
 * Synchronous 모델의 논리적 흐름
 * * 동기 모델의 핵심은 '결과를 기다리는 주체'가 애플리케이션 자신이라는 점입니다.
 * 프로세스 추상화 관점에서 보면, 이는 명령어의 순차적 실행을 
 * 논리적으로 강제하여 데이터의 정합성을 확보하는 가장 확실한 방법입니다.
 */
void synchronous_data_processing(int sockfd) {
    char buffer[1024];
    
    /* * 1. 작업의 요청과 인과적 결합:
     * 호출자는 커널에 데이터를 요청한 후, 해당 데이터가 유저 영역의 
     * 메모리에 완전히 복사될 때까지 이 작업의 컨텍스트를 유지합니다.
     */
    printf("동기적 데이터 획득 시작...\n");

    /* * 2. Blocking-Sync vs Non-Blocking-Sync:
     * - Blocking-Sync: 아래 read()에서 프로세스가 잠들며 결과를 기다림.
     * - Non-Blocking-Sync: EAGAIN을 받더라도 루프를 돌며 직접 결과를 확인(Polling).
     * * 어떤 경우든 '결과를 확인하고 확정 짓는 주체'가 애플리케이션이므로 
     * 이는 논리적으로 동기(Synchronous) 작업에 해당합니다.
     */
    ssize_t result = read(sockfd, buffer, sizeof(buffer));

    /* * 3. 결과의 즉각적 처리:
     * 커널로부터 제어권과 함께 '완성된 데이터'를 넘겨받는 순간 
     * 다음 로직으로 진행합니다. 프로세스의 선형적 타임라인이 유지되는 지점입니다.
     */
    if (result > 0) {
        process_received_data(buffer, result);
    }
    
    // 작업이 완전히 끝난 후에야 다음 비즈니스 로직이 실행됩니다.
    printf("동기적 작업 종결. 다음 단계로 이동합니다.\n");
    proceed_to_next_step();
}
```

동기 모델에서의 데이터 복사는 커널 스페이스와 유저 스페이스 사이의 엄격한 경계선에서 이루어집니다. 커널이 하드웨어로부터 데이터를 받아 자신의 버퍼를 채우는 동안, 애플리케이션은 '결과'라는 상태를 얻기 위해 대기하거나 주기적으로 상태를 확인합니다. CPU와 RAM은 마더보드를 통해 긴밀히 연결되어 데이터를 주고받지만, 동기식 I/O에서는 이 물리적 속도보다 커널의 작업 완료 통지가 우선시됩니다. 결국 커널 버퍼에서 유저 메모리로 데이터 복사가 완료되는 시점이 곧 동기적 작업의 종결점이 됩니다.

> **동기식 처리의 구조적 안정성**
> 
> 동기 모델은 호출자와 피호출자 사이의 상태가 항상 일치함을 보장합니다. 이는 복잡한 분산 시스템이나 다중 스레드 환경에서 발생할 수 있는 '상태 불일치' 문제를 원천 차단하며, 데이터의 순차적 정합성이 무엇보다 중요한 트랜잭션 처리 영역에서 시스템의 복잡도를 낮추고 안정성을 확보하는 결정적인 역할을 수행합니다.
{: .prompt-tip }

## 🧩 Asynchronous

Asynchronous(비동기) 모델은 '관심의 완전한 분리'를 통해 I/O 지연의 굴레를 벗어던집니다. 애플리케이션은 커널에 I/O 작업을 요청함과 동시에 그 작업에 대한 모든 권한과 책임을 넘깁니다. "데이터가 준비되면 나를 깨우거나, 내 메모리에 직접 넣어달라"는 계약을 맺는 것입니다. 진정한 비동기 모델에서 애플리케이션은 I/O 작업이 진행되는 동안 자신의 본래 업무를 수행하며, 커널이 배경(Background)에서 데이터를 모두 채운 뒤 통지(Notification)를 보내는 시점에만 반응합니다. 이는 CPU가 유휴 상태로 낭비되는 시간을 물리적으로 제거하려는 엔지니어링의 정수입니다.

이 모델의 정수는 커널이 유저 스페이스의 메모리에 데이터를 직접 써주는 'Proactor' 패턴에 있습니다. 기존의 동기 방식이 "데이터가 왔으니 가져가라"고 알려주는 수동적 통지였다면, 비동기는 "너의 메모리에 데이터를 이미 다 채워두었으니 바로 쓰기만 해라"고 확언합니다. CPU와 RAM 사이의 긴밀한 협력을 가장 극대화하는 방식입니다. 이는 유저 스페이스에서의 추가적인 데이터 복사 비용을 극단적으로 낮추며, 리눅스의 `io_uring`이나 윈도우의 `IOCP` 같은 기술들이 지향하는 궁극적인 I/O 효율성을 상징합니다.

![](/assets/img/2026-01-10-20-36-30.png)

이러한 제어의 역전, 즉 요청 시점과 완료 통지 시점이 완전히 분리되어 실행 흐름이 파편화되는 메커니즘은 아래의 코드 구조 속에 상세히 묘사되어 있습니다. 시스템 콜이 즉시 반환되는 것을 넘어, 작업의 종결 주체가 커널로 넘어가는 과정을 확인할 수 있습니다.

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <aio.h>

/**
 * Asynchronous I/O 인터페이스 모델
 * * 비동기 모델의 핵심은 '권한과 책임의 전적인 이양'입니다.
 * 이는 CPU가 I/O 장치의 응답을 기다리는 
 * 0.1%의 시간조차 허용하지 않고 오직 연산에만 몰입하게 만드는 방식입니다.
 */
void asynchronous_io_handler(sigval_t sig) {
    struct aiocb *cbp = (struct aiocb *)sig.sival_ptr;

    // 4. 완료 통지 및 처리 (Callback/Notification):
    // 커널이 유저 메모리에 데이터를 다 채운 뒤에야 비로소 이 함수가 호출됩니다.
    if (aio_error(cbp) == 0) {
        ssize_t n = aio_return(cbp);
        printf("\n[통지] 비동기 읽기 완료: %.*s\n", (int)n, (char *)cbp->aio_buf);
    }
}

void start_asynchronous_read(int sockfd) {
    struct aiocb cb;
    char buffer[1024];

    memset(&cb, 0, sizeof(struct aiocb));
    cb.aio_fildes = sockfd;
    cb.aio_buf = buffer;
    cb.aio_nbytes = sizeof(buffer);
    
    // 1. 통지 메커니즘 설정:
    // 데이터가 준비되면 호출될 콜백 함수를 지정합니다.
    cb.aio_sigevent.sigev_notify = SIGEV_THREAD;
    cb.aio_sigevent.sigev_notify_function = asynchronous_io_handler;
    cb.aio_sigevent.sigev_value.sival_ptr = &cb;

    /*
     * 2. 작업 요청 및 즉시 반환 (Request Submission):
     * aio_read()는 커널에게 '나중에 읽어서 내 메모리에 넣어달라'고 
     * 명령만 내린 뒤 0.0001초 만에 반환됩니다.
     */
    if (aio_read(&cb) == -1) {
        perror("aio_read 실패");
        return;
    }

    /*
     * 3. 독립적 업무 수행 (Non-dependent Execution):
     * 프로세스의 'Running' 상태를 100% 활용합니다.
     * I/O가 배경에서 돌아가는 동안, 메인 로직은 멈추지 않고 연산을 계속합니다.
     */
    for (int i = 0; i < 5; i++) {
        printf("메인 스레드는 다른 연산 중... [%d]\n", i);
        sleep(1); 
    }
}
```

그러나 비동기 모델은 '인지적 부하'라는 높은 청구서를 내밉니다. 작업의 요청과 완료가 시간적으로 격리되면서 프로그램의 논리적 선후 관계가 뒤섞이게 되고, 이는 스택 추적(Stack Trace)의 단절로 이어져 디버깅을 미로 속에 가둡니다. 묘사한 일관된 프로세스의 흐름이 비동기 세계에서는 수많은 콜백과 이벤트의 파편으로 흩어지게 됩니다. 엔지니어는 흩어진 파편들을 모아 하나의 비즈니스 컨텍스트로 엮어내기 위한 정교한 상태 머신(State Machine)을 구축해야 하는 과제를 떠안게 됩니다.

> **비동기 모델의 도입 비용과 가치**
> 
> 비동기 I/O는 커널의 잠재력을 극한까지 끌어올려 최소한의 자원으로 수만 건의 동시 처리를 가능케 하지만, 설계의 복잡도와 유지보수 비용 역시 기하급수적으로 상승합니다. 단순히 성능 수치에 매몰되기보다, 시스템의 규모와 팀의 숙련도, 그리고 해당 OS 환경이 제공하는 비동기 프리미티브의 성숙도를 면밀히 검토하여 도입 실익을 따져야 합니다.
{: .prompt-warning }

## Linux epoll

현대 고성능 서버의 역사는 'C10K 문제(만 개의 클라이언트 동시 접속)'를 어떻게 해결할 것인가에 대한 투쟁의 기록입니다. 과거의 `select`와 `poll`이 가졌던 근본적인 한계는 감시 대상인 소켓의 수가 늘어날수록 성능이 선형적으로 하락($O(N)$)한다는 점이었습니다. 운영체제는 하드웨어 자원을 추상화하여 관리하는데, 매번 수만 개의 리스트를 커널로 복사하고 처음부터 끝까지 훑어야 하는 방식은 CPU 연산 자원의 낭비를 넘어 시스템 전체의 응답 지연을 초래합니다.

이러한 비효율을 타파하기 위해 등장한 리눅스의 `epoll`은 관점을 완전히 바꿉니다. 문제를 '상태의 등록'과 '이벤트의 통지'로 분리한 것입니다.

| 특징                 | select / poll                      | epoll                              |
| :------------------- | :--------------------------------- | :--------------------------------- |
| **시간 복잡도**      | $O(N)$ (전체 루프 탐색)            | $O(1)$ (이벤트 발생 건만 처리)     |
| **관심 리스트 관리** | 매번 유저 ↔ 커널 복사              | 커널 내부(Red-Black Tree)에서 관리 |
| **최대 소켓 수**     | FD_SETSIZE 제한 (기본 1024)        | 제한 없음 (시스템 자원 허용치까지) |
| **알림 방식**        | "어딘가 데이터 왔으니 너가 찾아봐" | "이 소켓들에 데이터 왔으니 가져가" |

`epoll`은 커널 내부에 **레드-블랙 트리(Red-Black Tree)**를 구축하여 감시 대상 소켓을 관리합니다. 소켓 하나가 추가되거나 삭제될 때 트리 구조를 통해 로그 시간 복잡도로 빠르게 상태를 변경하며, 데이터가 실제로 도착한 소켓들만 별도의 **Ready List(링크드 리스트)**에 담습니다. CPU와 RAM 사이의 긴밀한 협력은 여기서 빛을 발합니다. 하드웨어 인터럽트가 발생하면 커널은 인터럽트 핸들러를 통해 Ready List에 해당 소켓을 즉시 집어넣고, 애플리케이션은 이 리스트만 읽어 가면 되기 때문입니다.

![](/assets/img/2026-01-13-16-35-40.png)

여기서 엔지니어가 반드시 결정해야 할 트레이드오프는 **Trigger 방식**의 선택입니다.

* **Level-Triggered (LT)**: 데이터가 버퍼에 남아 있는 동안 계속 이벤트를 발생시킵니다. `select`와 유사하여 구현이 쉽지만, 불필요한 이벤트 호출이 잦을 수 있습니다.
* **Edge-Triggered (ET)**: 데이터가 처음 도착한 '변화의 순간'에만 딱 한 번 이벤트를 발생시킵니다. Non-Blocking 소켓과 결합하여 CPU 유휴 시간을 극한으로 쥐어짤 수 있지만, 버퍼를 완전히 비우지 않으면 영영 통지를 받지 못할 수 있는 위험이 있어 정교한 설계가 필요합니다.

```c
#include <sys/epoll.h>
#include <unistd.h>
#include <stdio.h>
#include <fcntl.h>

/**
 * Linux epoll을 활용한 이벤트 루프 구현
 * * epoll의 핵심은 '관심의 등록'과 '통지의 수신'을 분리하는 데 있습니다.
 * 운영체제는 하드웨어 자원을 효율적으로 관리하기 위해 
 * 프로세스 상태 전이를 제어하며, epoll은 이 과정에서 발생하는 
 * 컨텍스트 스위칭과 데이터 복사 비용을 극한으로 절감합니다.
 */
void run_epoll_event_loop(int server_fd) {
    // 1. epoll 인스턴스 생성
    // 커널 내부에 이벤트를 관리하기 위한 전용 공간(레드-블랙 트리)을 마련합니다.
    int epfd = epoll_create1(0);
    struct epoll_event ev, events[MAX_EVENTS];

    // 2. 엣지 트리거(Edge-Triggered) 및 Non-Blocking 설정
    // 변화가 일어나는 '순간'에만 통지받아 시스템 콜 호출 횟수를 최소화합니다.
    set_nonblocking(server_fd);
    ev.events = EPOLLIN | EPOLLET; 
    ev.data.fd = server_fd;

    // 3. 관심 리스트 등록 (epoll_ctl)
    // 커널 내부 트리 구조에 소켓을 등록합니다. 이후 소켓 상태 변경은 
    // 커널이 직접 관리하므로 매번 전체 리스트를 전달할 필요가 없습니다.
    epoll_ctl(epfd, EPOLL_CTL_ADD, server_fd, &ev);

    while (1) {
        /**
         * 4. 이벤트 대기 (epoll_wait)
         * 프로세스는 Ready List에 데이터가 찰 때까지 효율적으로 대기합니다.
         * CPU는 이 기간 동안 다른 프로세스를 처리할 수 있으며, 
         * Ready List에 데이터가 들어오는 순간 $O(1)$의 속도로 즉시 깨어납니다.
         */
        int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);

        for (int n = 0; n < nfds; ++n) {
            if (events[n].data.fd == server_fd) {
                // 새로운 연결 수락 로직
                accept_new_connection(epfd, server_fd);
            } else {
                // 데이터 읽기 및 비즈니스 로직 수행
                handle_client_data(events[n].data.fd);
            }
        }
    }
}
```

> **epoll이 달성한 기술적 도약**
> 
> `epoll`은 단순히 속도를 높인 것이 아니라, 유저 스페이스와 커널 스페이스 사이의 '불필요한 대화'를 제거했습니다. 프로세스 컨텍스트 스위칭은 비싼 비용을 치르는데, `epoll`은 소켓 상태를 확인하기 위한 스위칭 횟수와 데이터 복사량을 최소화함으로써 하드웨어 성능을 소프트웨어 수준에서 온전히 누릴 수 있게 만들었습니다.
{: .prompt-tip }

## Java NIO

Java 엔지니어에게 Linux의 `epoll`은 강력하지만 다루기 까다로운 원시 도구와 같습니다. Java NIO(New I/O)는 이 거친 커널의 인터페이스를 객체지향의 세계로 끌어올려, 단일 스레드가 수천 개의 연결을 효율적으로 관리할 수 있는 **Selector, Channel, Buffer**라는 삼각 편대의 추상화를 제공합니다. 운영체제는 하드웨어 자원을 추상화하여 제공하며, Java NIO는 바로 이 지점에서 커널의 성능을 자바 가상 머신(JVM) 위에서 온전히 구현해냅니다.

Java NIO의 정수는 **Selector**에 있습니다. 이는 Linux `epoll`의 자바판 래퍼(Wrapper)로, 수많은 **Channel**들 중 I/O 이벤트가 발생한 것들만 골라내는 멀티플렉서 역할을 수행합니다. 과거 Blocking I/O 방식이 '연결당 스레드 하나'를 소모하며 컨텍스트 스위칭 비용과 메모리 부족 문제를 야기했다면, NIO는 단 하나의 스레드만으로 수만 개의 소켓을 감시합니다.

![](/assets/img/2026-01-10-20-36-48.png)

데이터의 이동 역시 혁신적입니다. Java NIO는 **Direct Buffer**를 통해 'Zero-copy'에 가까운 성능을 구현합니다. 일반적으로 데이터는 하드웨어에서 커널 버퍼로, 다시 JVM 힙 메모리로 복사되는 과정을 거치지만, Direct Buffer는 JVM 힙을 거치지 않고 커널 메모리와 직접 소통합니다. CPU와 RAM 사이의 마더보드 통로를 가장 짧은 경로로 활용하는 것입니다.

```java
import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.*;
import java.util.Iterator;
import java.util.Set;

/**
 * Java NIO Selector를 이용한 비차단 서버 루프
 * * Java NIO는 Linux의 epoll과 같은 저수준 커널 인터페이스를 객체지향적으로 추상화합니다.
 * 이는 단일 스레드가 수천 개의 프로세스/스레드를 대신하여
 * 자원을 효율적으로 관리하게 하는 '멀티플렉싱'의 정수입니다.
 */
public class NioServer {
    public void startServer() throws IOException {
        // 1. Selector 및 ServerSocketChannel 오픈
        // Selector는 커널의 epoll 인스턴스를 관리하는 자바 측 대리인입니다.
        Selector selector = Selector.open();
        ServerSocketChannel serverSocket = ServerSocketChannel.open();
        serverSocket.bind(new InetSocketAddress("localhost", 8080));
        
        // 2. 비차단(Non-blocking) 설정
        // 이 설정이 있어야만 Selector가 여러 채널을 동시에 감시할 수 있습니다.
        serverSocket.configureBlocking(false);

        // 3. 관심 이벤트 등록 (ACCEPT)
        // 커널의 관심 리스트(Red-Black Tree)에 이 서버 소켓을 등록하는 과정입니다.
        serverSocket.register(selector, SelectionKey.OP_ACCEPT);

        System.out.println("NIO 서버가 시작되었습니다. 이벤트를 감시합니다...");

        while (true) {
            /**
             * 4. 이벤트 감시 (select)
             * 이 지점에서 스레드는 Ready List에 신호가 올 때까지 대기하며 CPU 자원을 반납합니다.
             */
            selector.select(); 

            // 5. 발생한 이벤트(SelectionKey) 세트 획득
            Set<SelectionKey> selectedKeys = selector.selectedKeys();
            Iterator<SelectionKey> iter = selectedKeys.iterator();

            while (iter.hasNext()) {
                SelectionKey key = iter.next();

                if (key.isAcceptable()) {
                    // 신규 연결 수락 및 비차단 등록
                    registerClient(selector, serverSocket);
                } else if (key.isReadable()) {
                    // 데이터 읽기 수행
                    // Direct Buffer 등을 사용하면
                    // 유저/커널 사이의 데이터 복사 비용을 'Zero-copy'에 가깝게 줄일 수 있습니다.
                    readData(key);
                }
                iter.remove(); // 처리한 이벤트는 명시적으로 제거
            }
        }
    }

    private void registerClient(Selector selector, ServerSocketChannel serverSocket) throws IOException {
        SocketChannel client = serverSocket.accept();
        client.configureBlocking(false);
        client.register(selector, SelectionKey.OP_READ);
    }

    private void readData(SelectionKey key) throws IOException {
        SocketChannel client = (SocketChannel) key.channel();
        ByteBuffer buffer = ByteBuffer.allocate(1024);
        int bytesRead = client.read(buffer);

        if (bytesRead == -1) {
            client.close();
        } else {
            // 고트래픽 환경에서는 이 데이터 처리를 별도의 워커 스레드 풀로 넘겨 
            // Selector 스레드의 지연을 방지하는 것이 고성능 아키텍처의 정석입니다.
            buffer.flip();
            System.out.println("데이터 수신: " + new String(buffer.array()).trim());
        }
    }
}
```

> **고성능 추상화 이면에 숨겨진 제어의 복잡성**
> 
> Java NIO는 성능 면에서 압도적이지만, 버퍼의 `flip()`, `clear()` 처리나 소켓의 상태 관리 등 개발자가 신경 써야 할 저수준의 복잡도가 매우 높습니다. 이러한 이유로 실제 현업에서는 NIO를 직접 다루기보다, 이를 한 단계 더 우아하게 감싸 안은 **Netty** 같은 프레임워크를 사용하여 Reactor 패턴을 구현하는 것이 일반적입니다. 결국 기술의 선택은 '성능 확보'와 '유지보수 생산성' 사이에서 최적의 균형점을 찾아내는 과정입니다.
{: .prompt-warning }

## Summary

I/O 모델은 현대 고성능 시스템 아키텍처를 지탱하는 가장 근본적인 기둥입니다. Blocking의 직관성에서 시작해 Non-Blocking의 자유를 거쳐, 리눅스 `epoll`과 Java NIO라는 거대한 성능 최적화의 정점에 이르기까지 엔지니어는 끊임없이 CPU 유휴 시간을 줄이고 자원 활용도를 높이는 투쟁을 이어왔습니다. 성능의 임계점은 결국 커널과 유저 스페이스 사이의 데이터 흐름과 문맥 전환 비용을 얼마나 정교하게 통제하느냐에 달려 있습니다.

## References

* [[PlanetScale] Processes and Threads](https://planetscale.com/blog/processes-and-threads)
* [[Linux Manual] epoll(7) - I/O Event Notification Facility](https://man7.org/linux/man-pages/man7/epoll.7.html)
* [[Baeldung] Guide to Java NIO Selector](https://www.baeldung.com/java-nio-selector)